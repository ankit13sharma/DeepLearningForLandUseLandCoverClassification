{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_multiclass_18-10-20.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ounkna8hcEAS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c7c3a37-24a6-4095-baa3-045e1df8b095"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWC7ZBjvi4h0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "962fd931-de54-4890-d0b2-e9eb50eeca1f"
      },
      "source": [
        "pip install h2o4gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting h2o4gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/26/a7c66b98828a0d7998e7b340eb6a167c641cbc4ec5e63dbc4e338fe992f8/h2o4gpu-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (346.3MB)\n",
            "\u001b[K     |████████████████████████████████| 346.3MB 48kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.4 in /usr/local/lib/python3.6/dist-packages (from h2o4gpu) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from h2o4gpu) (0.16.0)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.6/dist-packages (from h2o4gpu) (1.1.2)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from h2o4gpu) (0.16.0)\n",
            "Collecting psutil>=5.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/3e/d18f2c04cf2b528e18515999b0c8e698c136db78f62df34eee89cee205f1/psutil-5.7.2.tar.gz (460kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 49.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from h2o4gpu) (1.18.5)\n",
            "Requirement already satisfied: tabulate>=0.8.2 in /usr/local/lib/python3.6/dist-packages (from h2o4gpu) (0.8.7)\n",
            "Collecting scikit-learn==0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/04/49633f490f726da6e454fddc8e938bbb5bfed2001681118d3814c219b723/scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from h2o4gpu) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from h2o4gpu) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.2->h2o4gpu) (1.15.0)\n",
            "Building wheels for collected packages: psutil\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.7.2-cp36-cp36m-linux_x86_64.whl size=279868 sha256=6b952b7eb270d010d385e52424277851adb6a3f3aa43f9d3e4b4e01b5a36a81d\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/a0/f5/c4fa280463e29aea07797acb5312358fefb067c1f4f98e11b1\n",
            "Successfully built psutil\n",
            "Installing collected packages: psutil, scikit-learn, h2o4gpu\n",
            "  Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed h2o4gpu-0.4.1 psutil-5.7.2 scikit-learn-0.21.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi4TowLVzHUA"
      },
      "source": [
        "from osgeo import gdal\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#!pip install tf-nightly-gpu-2.0-preview\n",
        "\n",
        "from time import time\n",
        "\n",
        "import h5py\n",
        "import random\n",
        "from scipy import ndarray\n",
        "from skimage import exposure\n",
        "#import skimage as sk\n",
        "#from skimage import transform\n",
        "#from skimage import util\n",
        "#from skimage.exposure import adjust_sigmoid, is_low_contrast\n",
        "from random import random as rd\n",
        "import cv2 as cv\n",
        "#from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator #, load_img, img_to_array\n",
        "#from keras.applications.vgg16 import preprocess_input #, decode_predictions, VGG16\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras.losses import binary_crossentropy as bce\n",
        "from keras.losses import categorical_crossentropy as cce\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping #,TensorBoard#, ReduceLROnPlateau, LearningRateScheduler\n",
        "from keras.utils.np_utils import to_categorical   \n",
        "from h2o4gpu.solvers.kmeans import KMeans as kmeans\n",
        "##categorical_labels = to_categorical(int_labels, num_classes=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f19R2wWzfhb"
      },
      "source": [
        "'''\n",
        "import datetime, os\n",
        "\n",
        "logs_base_dir = \"./logs\"\n",
        "os.makedirs(logs_base_dir, exist_ok=True)\n",
        "'''\n",
        "K.clear_session()\n",
        "\n",
        "\n",
        "\n",
        "def shuffle_list(*ls, seeds =78):\n",
        "  random.seed(seeds)\n",
        "  l =list(zip(*ls))\n",
        "\n",
        "  random.shuffle(l)\n",
        "  return zip(*l)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def raster2array(raster, bnd,x,y,wndo):\n",
        "    rr = np.zeros((wndo, wndo, bnd))\n",
        "    newValue = 0\n",
        "    for b in range(bnd):\n",
        "        band = raster.GetRasterBand(b+1)\n",
        "        rr[:,:,b] = band.ReadAsArray(x, y, wndo, wndo)\n",
        "        #noDataValue = band.GetNoDataValue()\n",
        "    #rr[rr == noDataValue] = newValue\n",
        "    return rr.astype(np.uint8)\n",
        "ipath = []\n",
        "cpath = []\n",
        "\n",
        "def sorted_aphanumeric(data):\n",
        "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
        "    return sorted(data, key=alphanum_key)\n",
        "  \n",
        "  \n",
        "def get_all_images(folder, ext):\n",
        "\n",
        "    all_files = []\n",
        "    #Iterate through all files in folder\n",
        "    for file in sorted_aphanumeric(os.listdir(folder)):\n",
        "        #Get the file extension\n",
        "        _,  file_ext = os.path.splitext(file)\n",
        "\n",
        "        #If file is of given extension, get it's full path and append to list\n",
        "        if ext in file_ext:\n",
        "            full_file_path = os.path.join(folder, file)\n",
        "            all_files.append(full_file_path)\n",
        "\n",
        "    #Get list of all files\n",
        "    return all_files\n",
        "\n",
        "\n",
        "def coord(ipath, nm, hmargin, wmargin, steps):\n",
        "    raster = gdal.Open(ipath)\n",
        "\n",
        "\n",
        "    w,h = round(raster.RasterXSize), round(raster.RasterYSize)\n",
        "    print(h,w)\n",
        "    raster = None\n",
        "    #margin = 0\n",
        "    size = 256\n",
        "    ar1= (np.arange(hmargin,h+1,size))\n",
        "    ar1 = ar1[:-1]\n",
        "    #print(ar1)\n",
        "    ar2= np.arange(wmargin,w+1,size)\n",
        "    ar2 = ar2[:-1]\n",
        "    #print(ar2)\n",
        "\n",
        "    i= (ar1.shape[0]) \n",
        "    j = (ar2.shape[0])\n",
        "    l = i*j\n",
        "    print(i,j,l)\n",
        "    print(ar1.shape)\n",
        "    print(ar2.shape)\n",
        "    ar2 = ar2*np.ones((i,j))\n",
        "    ar1 = ar1.reshape(i,1)*(np.ones((i,j)))\n",
        "    ind = np.arange(0,l,1).reshape(i,j)\n",
        "    idj = np.zeros_like(ind)\n",
        "    idj = idj + nm\n",
        "    ar1 = np.around(ar1)\n",
        "    ar2 = np.around(ar2)\n",
        "\n",
        "\n",
        "    step = steps\n",
        "    ind = list(ind.ravel()[::step])\n",
        "    ar1 = list(ar1.ravel()[::step])\n",
        "    ar2 = list(ar2.ravel()[::step])\n",
        "    idj = list(idj.ravel()[::step])\n",
        "\n",
        "    return (idj,ind,ar1,ar2)\n",
        "\n",
        "\n",
        "def rdata(start_w, start_h, w,h, wmargin, hmargin, number, i):\n",
        "    \n",
        "    np.random.seed(440+i)\n",
        "    a1 = list(np.squeeze(np.random.randint(start_h,(h-hmargin),size=(number,1)).astype(np.float64)))\n",
        "    np.random.seed(494+i)\n",
        "    a2 = list(np.squeeze(np.random.randint(start_w,(w-wmargin),size=(number,1)).astype(np.float64)))\n",
        "    in1 = list(np.squeeze(np.arange(0,number,1).reshape(number,1)))\n",
        "    id1 = list(np.squeeze(np.zeros((number,1),dtype=np.uint8)+i))\n",
        "    ##id1 = list(np.squeeze(np.zeros((number,1),dtype=np.uint8)+i))\n",
        "\n",
        "    return (id1, in1, a1, a2)\n",
        "\n",
        "def apply_rdata(filepath,idj1,ind1,ar11,ar21,number, unique_id):\n",
        "    raster = gdal.Open(filepath)\n",
        "    w,h = round(raster.RasterXSize), round(raster.RasterYSize)\n",
        "    #print(h,w)\n",
        "    raster = None\n",
        "\n",
        "    idj2,ind2,ar12,ar22 = rdata(0,0, w, h,size,size,number,unique_id)   \n",
        "\n",
        "    print('idj: ', len(idj2))\n",
        "    print('ar1: ', len(ar12))\n",
        "    print('ar2: ', len(ar22))\n",
        "    \n",
        "    idj1 = idj1 + idj2\n",
        "    ind1 = ind1 + ind2\n",
        "    ar11 = ar11 + ar12\n",
        "    ar21 = ar21 + ar22\n",
        "\n",
        "    return idj1,ind1, ar11, ar21\n",
        "\n",
        "\n",
        "def apply_coord(filepath,idj1,ind1,ar11,ar21,steps, unique_id):\n",
        "    raster = gdal.Open(filepath)\n",
        "    w,h = round(raster.RasterXSize), round(raster.RasterYSize)\n",
        "    #print(h,w)\n",
        "    raster = None\n",
        "\n",
        "    idj2,ind2,ar12,ar22 = coord(filepath,unique_id,0,0,steps)   \n",
        "    \n",
        "    print('idj: ', len(idj2))\n",
        "    print('ar1: ', len(ar12))\n",
        "    print('ar2: ', len(ar22))\n",
        "    \n",
        "    \n",
        "    idj1 = idj1 + idj2\n",
        "    ind1 = ind1 + ind2\n",
        "    ar11 = ar11 + ar12\n",
        "    ar21 = ar21 + ar22\n",
        "\n",
        "    return idj1,ind1, ar11, ar21\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fGiIgSdzkMn"
      },
      "source": [
        "# Convert Raster to array\n",
        "\n",
        "def img_generator(ipathi,cpathi,idjx,indx,ar1x,ar2x, batch):\n",
        "    \n",
        "    def generate_labels(image,ndvi,ndwi):\n",
        "        img1 = process_kmeans2(image)\n",
        "        img2 = np.zeros_like(img1)\n",
        "        clt = norm_classes(img1,img2,ndvi,ndwi)\n",
        "        return clt\n",
        "\n",
        "\n",
        "    def process_kmeans2(imgn, K=4):\n",
        "        criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 0.05)\n",
        "        # Set flags (Just to avoid line break in the code)\n",
        "        flags = cv.KMEANS_RANDOM_CENTERS  \n",
        "        img1 = imgn.reshape((-1,imgn.shape[-1]))\n",
        "        img1 = np.float32(img1)\n",
        "        # compactness,label,center = cv.kmeans(img1, K,None,criteria,30,flags)\n",
        "        kmn = kmeans(n_clusters=4, n_init=30, max_iter=10, tol=0.01, precompute_distances='auto', verbose=0, random_state=36, copy_x=True, n_jobs=None, algorithm='auto').fit(img1)\n",
        "        label = kmn.labels_\n",
        "        return label.reshape((imgn.shape[0],imgn.shape[1]))\n",
        "\n",
        "    def norm_classes(img1,img2,ndvi,ndwi):\n",
        "        for c in list(set(img1.flatten())):    \n",
        "            mask = np.zeros_like(img1)\n",
        "            mask[img1==c] =1\n",
        "            mean_ndvi = np.sum(ndvi[img1==c])/np.sum(mask)\n",
        "            mean_ndwi = np.sum(ndwi[img1==c])/np.sum(mask)\n",
        "            if (mean_ndvi<0.2 and mean_ndvi>0 and mean_ndwi<0):\n",
        "                img2[img1==c] = 7\n",
        "            elif (mean_ndvi<0.4 and mean_ndvi>=0.2 and mean_ndwi<0):\n",
        "                img2[img1==c] = 8\n",
        "            elif (mean_ndvi>=0.4 and mean_ndwi<0):\n",
        "                img2[img1==c] = 9\n",
        "            elif (mean_ndvi<0 and mean_ndwi>=0.3):\n",
        "                img2[img1==c] = 1 \n",
        "            elif (mean_ndvi<0 and mean_ndwi<0.3 and mean_ndwi>0):\n",
        "                img2[img1==c] = 2 \n",
        "            elif (mean_ndvi<0 and mean_ndwi<0):\n",
        "                img2[img1==c] = 3\n",
        "            elif (mean_ndvi<=0.2 and mean_ndvi>0 and mean_ndwi<=0.3 and mean_ndwi>0):\n",
        "                img2[img1==c] = 4 \n",
        "            elif (mean_ndvi<=0.4 and mean_ndvi>0.2 and mean_ndwi<=0.3 and mean_ndwi>0):\n",
        "                img2[img1==c] = 5 \n",
        "            elif (mean_ndvi==0 and mean_ndwi==0):\n",
        "                img2[img1==c] = 0\n",
        "            else:\n",
        "                img2[img1==c] = 6\n",
        "        return img2\n",
        "\n",
        "    def add_veg_water_index(image):\n",
        "        h,w = image.shape[:2]\n",
        "        image2= np.zeros((h,w,6))\n",
        "        image2[:,:,:4] = image\n",
        "        del image\n",
        "        r = image2[:,:,2]\n",
        "        g = image2[:,:,1]\n",
        "        nir = image2[:,:,3]\n",
        "        \n",
        "        num1 = nir-r\n",
        "        dnm1 = nir+r\n",
        "        num2= g-nir\n",
        "        dnm2 = g+nir\n",
        "        \n",
        "        del nir,r,g\n",
        "        ndvi = np.divide(num1,dnm1, where = (dnm1!=0), dtype = np.float64)\n",
        "        ndwi = np.divide(num2,dnm2, where = (dnm2!=0), dtype = np.float64)\n",
        "        \n",
        "        del num1,num2,dnm1,dnm2\n",
        "        ndvi[ndvi>1] = 0\n",
        "        ndvi[ndvi<-1] = 0\n",
        "        ndwi[ndwi>1] = 0\n",
        "        ndwi[ndwi<-1] = 0\n",
        "        \n",
        "        image2[:,:,4] = 127.5 + ndvi*127.5\n",
        "        image2[:,:,5] = 127.5 + ndwi*127.5\n",
        "        \n",
        "        return image2,ndvi,ndwi\n",
        "\n",
        "    def raster2arr(raster, bnd,x,y,wndo):\n",
        "        rr = np.zeros((wndo, wndo, bnd))\n",
        "        newValue = 0\n",
        "        for b in range(bnd):\n",
        "            band = raster.GetRasterBand(b+1)\n",
        "            img = band.ReadAsArray(x, y, wndo, wndo)\n",
        "            noDataValue = band.GetNoDataValue()\n",
        "            img[img == noDataValue] = newValue\n",
        "            rr[:,:,b] = img\n",
        "        return rr.astype(np.uint8)\n",
        "    \n",
        "    def contrast(image_array: ndarray):\n",
        "            v_min, v_max = np.percentile(image_array, (0.2, 99.8))\n",
        "            return exposure.rescale_intensity(image_array, in_range=(v_min, v_max))\n",
        "\n",
        "    def com_gen(image,label):\n",
        "        K.set_image_data_format('channels_last')\n",
        "        def combine_generator(gen1, gen2):\n",
        "            while True:\n",
        "                yield(gen1.next(), gen2.next())\n",
        "        \n",
        "        data_gen_args = dict(horizontal_flip=True,\n",
        "                         vertical_flip=True,\n",
        "                         rotation_range=45,\n",
        "                         width_shift_range=0.2,\n",
        "                         height_shift_range=0.2,\n",
        "                         #brightness_range=[0.8,1.2],\n",
        "                         #shear_range=0.2,\n",
        "                         #channel_shift_range = 20,\n",
        "                         zoom_range=0.2,\n",
        "                         fill_mode='nearest',\n",
        "                         data_format = \"channels_last\",\n",
        "                         rescale = 1./255)\n",
        "        '''\n",
        "        data_gen_args = dict(data_format = \"channels_last\",\n",
        "                             rescale = 1./255)\n",
        "        '''\n",
        "        image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "        mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "        #image_datagen.fit(image, augment=True, rounds=1, seed=42)\n",
        "        #mask_datagen.fit(label, augment=True, rounds=1, seed=42)\n",
        "        image_generator = image_datagen.flow(image, batch_size=1, seed=42)\n",
        "        mask_generator = mask_datagen.flow(label, batch_size=1, seed=42)\n",
        "        train_generator = combine_generator(image_generator,mask_generator)\n",
        "        return train_generator\n",
        "\n",
        "    \n",
        "    ntr = len(ar1x)\n",
        "    q =list(range(ntr))\n",
        "    random.seed(948)\n",
        "    random.shuffle(q)\n",
        "    \n",
        "    num = 0\n",
        "    while (num<ntr):\n",
        "        \n",
        "        random.shuffle(q)\n",
        "        \n",
        "        batch_input = np.zeros((batch,size,size,6))\n",
        "        batch_output = np.zeros((batch,size,size,10))\n",
        "        #ind1,ar11,ar21 = shuffle_lst(ind1,ar11,ar21)\n",
        "       \n",
        "        for m in range(batch):\n",
        "            ip = q[num]\n",
        "            x = ar2x[ip]\n",
        "            y = ar1x[ip]\n",
        "            ijx = idjx[ip]\n",
        "            \n",
        "        \n",
        "            raster = gdal.Open(ipathi[ijx])        \n",
        "            img = raster2arr(raster,4,x,y,size)\n",
        "            raster = None\n",
        "            batch_input[m,:,:,:],ndvi,ndwi = add_veg_water_index(img)\n",
        "            clt = generate_labels(batch_input[m,:,:,:],ndvi,ndwi)\n",
        "            \n",
        "            # raster = gdal.Open(cpathi[ijx])\n",
        "             \n",
        "            # clt = raster2arr(raster,1,x,y,size)\n",
        "            # raster = None\n",
        "            batch_output[m,:,:,:] = (to_categorical(clt,num_classes=10))*255.0\n",
        "            del img,clt\n",
        "        \n",
        "        num += 1\n",
        "        return (com_gen(batch_input, batch_output))\n",
        "        #return (com_gen(contrast(batch_input), batch_output))\n",
        "\n",
        "def msk_generator(ipathm,cpathm,idjy,indy,ar1y,ar2y, batch):    \n",
        "    \n",
        "    def generate_labels(image,ndvi,ndwi):\n",
        "        img1 = process_kmeans2(image)\n",
        "        img2 = np.zeros_like(img1)\n",
        "        clt = norm_classes(img1,img2,ndvi,ndwi)\n",
        "        return clt\n",
        "\n",
        "    def process_kmeans2(imgn, K=4):\n",
        "        criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 0.05)\n",
        "        # Set flags (Just to avoid line break in the code)\n",
        "        flags = cv.KMEANS_RANDOM_CENTERS  \n",
        "        img1 = imgn.reshape((-1,imgn.shape[-1]))\n",
        "        img1 = np.float32(img1)\n",
        "        # compactness,label,center = cv.kmeans(img1, K,None,criteria,30,flags)\n",
        "        kmn = kmeans(n_clusters=4, n_init=30, max_iter=10, tol=0.01, precompute_distances='auto', verbose=0, random_state=36, copy_x=True, n_jobs=None, algorithm='auto').fit(img1)\n",
        "        label = kmn.labels_\n",
        "        return label.reshape((imgn.shape[0],imgn.shape[1]))\n",
        "\n",
        "    def norm_classes(img1,img2,ndvi,ndwi):\n",
        "        for c in list(set(img1.flatten())):    \n",
        "            mask = np.zeros_like(img1)\n",
        "            mask[img1==c] =1\n",
        "            mean_ndvi = np.sum(ndvi[img1==c])/np.sum(mask)\n",
        "            mean_ndwi = np.sum(ndwi[img1==c])/np.sum(mask)\n",
        "            if (mean_ndvi<0.2 and mean_ndvi>0 and mean_ndwi<0):\n",
        "                img2[img1==c] = 7\n",
        "            elif (mean_ndvi<0.4 and mean_ndvi>=0.2 and mean_ndwi<0):\n",
        "                img2[img1==c] = 8\n",
        "            elif (mean_ndvi>=0.4 and mean_ndwi<0):\n",
        "                img2[img1==c] = 9\n",
        "            elif (mean_ndvi<0 and mean_ndwi>=0.3):\n",
        "                img2[img1==c] = 1 \n",
        "            elif (mean_ndvi<0 and mean_ndwi<0.3 and mean_ndwi>0):\n",
        "                img2[img1==c] = 2 \n",
        "            elif (mean_ndvi<0 and mean_ndwi<0):\n",
        "                img2[img1==c] = 3\n",
        "            elif (mean_ndvi<=0.2 and mean_ndvi>0 and mean_ndwi<=0.3 and mean_ndwi>0):\n",
        "                img2[img1==c] = 4 \n",
        "            elif (mean_ndvi<=0.4 and mean_ndvi>0.2 and mean_ndwi<=0.3 and mean_ndwi>0):\n",
        "                img2[img1==c] = 5 \n",
        "            elif (mean_ndvi==0 and mean_ndwi==0):\n",
        "                img2[img1==c] = 0\n",
        "            else:\n",
        "                img2[img1==c] = 6\n",
        "        return img2\n",
        "\n",
        "    def add_veg_water_index(image):\n",
        "        h,w = image.shape[:2]\n",
        "        image2= np.zeros((h,w,6))\n",
        "        image2[:,:,:4] = image\n",
        "        del image\n",
        "        r = image2[:,:,2]\n",
        "        g = image2[:,:,1]\n",
        "        nir = image2[:,:,3]\n",
        "        \n",
        "        num1 = nir-r\n",
        "        dnm1 = nir+r\n",
        "        num2= g-nir\n",
        "        dnm2 = g+nir\n",
        "        \n",
        "        del nir,r,g\n",
        "        ndvi = np.divide(num1,dnm1, where = (dnm1!=0), dtype = np.float64)\n",
        "        ndwi = np.divide(num2,dnm2, where = (dnm2!=0), dtype = np.float64)\n",
        "        \n",
        "        del num1,num2,dnm1,dnm2\n",
        "        ndvi[ndvi>1] = 0\n",
        "        ndvi[ndvi<-1] = 0\n",
        "        ndwi[ndwi>1] = 0\n",
        "        ndwi[ndwi<-1] = 0\n",
        "        \n",
        "        image2[:,:,4] = 127.5 + ndvi*127.5\n",
        "        image2[:,:,5] = 127.5 + ndwi*127.5\n",
        "        \n",
        "        return image2,ndvi,ndwi\n",
        "\n",
        "\n",
        "    def raster2arr(raster, bnd,x,y,wndo):\n",
        "        rr = np.zeros((wndo, wndo, bnd))\n",
        "        newValue = 0\n",
        "        for b in range(bnd):\n",
        "            band = raster.GetRasterBand(b+1)\n",
        "            img = band.ReadAsArray(x, y, wndo, wndo)\n",
        "            noDataValue = band.GetNoDataValue()\n",
        "            img[img == noDataValue] = newValue\n",
        "            rr[:,:,b] = img\n",
        "        return rr.astype(np.uint8)\n",
        "\n",
        "    def com_gen(image2,label2):\n",
        "        K.set_image_data_format('channels_last')\n",
        "        def combine_generator(gen1, gen2):\n",
        "            while True:\n",
        "                yield(gen1.next(), gen2.next())                                   \n",
        "  \n",
        "        data_gen_args2 = dict(data_format = \"channels_last\",\n",
        "                             rescale = 1./255)\n",
        "        image_datagen2 = ImageDataGenerator(**data_gen_args2)\n",
        "        mask_datagen2 = ImageDataGenerator(**data_gen_args2)\n",
        "        #image_datagen.fit(image, augment=True, rounds=5, seed=42)\n",
        "        #mask_datagen.fit(label, augment=True, rounds=5, seed=42)\n",
        "        image_generator2 = image_datagen2.flow(image2, batch_size=1, seed=56)\n",
        "        mask_generator2 = mask_datagen2.flow(label2, batch_size=1, seed=56)\n",
        "        val_generator = combine_generator(image_generator2,mask_generator2)\n",
        "        return val_generator\n",
        "    \n",
        "    ntr = len(ar1y)\n",
        "    q = list(range(ntr))\n",
        "    random.seed(493)\n",
        "    random.shuffle(q)\n",
        "        \n",
        "    num = 0\n",
        "    while (num<ntr):\n",
        "        \n",
        "        #random.shuffle(q)\n",
        "        \n",
        "        batch_inputm = np.zeros((batch,size,size,6))\n",
        "        batch_outputm = np.zeros((batch,size,size,10))\n",
        "        #ind1,ar11,ar21 = shuffle_lst(ind1,ar11,ar21)\n",
        "       \n",
        "        for m in range(batch):\n",
        "            ip = q[num]\n",
        "            x = ar2y[ip]\n",
        "            y = ar1y[ip]\n",
        "            ijy = idjy[ip]\n",
        "            \n",
        "            raster = gdal.Open(ipathm[ijy])     \n",
        "            img = raster2arr(raster,4,x,y,size)\n",
        "            raster = None           \n",
        "            batch_inputm[m,:,:,:],ndvi,ndwi = add_veg_water_index(img)\n",
        "            clt = generate_labels(batch_inputm[m,:,:,:],ndvi,ndwi)\n",
        "            # raster = gdal.Open(cpathm[ijy])\n",
        "             \n",
        "            # clt = raster2arr(raster,1,x,y,size)\n",
        "            # raster = None  \n",
        "            batch_outputm[m,:,:,:] = (to_categorical(clt,num_classes=10))*255.0  \n",
        "            del img,clt\n",
        "    \n",
        "        num += 1\n",
        "        return (com_gen(batch_inputm, batch_outputm))\n",
        "        #yield (batch_inputm, batch_outputm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-6vp2y9zoiT"
      },
      "source": [
        "def iou(y_true, y_pred):\n",
        "    smooth = 1e-12\n",
        "    '''\n",
        "    size =int(256)\n",
        "    y_true = K.cast(K.reshape(y_true, (-1,size,size)),np.uint8)\n",
        "    y_true = K.one_hot(y_true,3)\n",
        "    '''\n",
        "    intersection = K.sum((y_true * y_pred), axis = (0,1,2))\n",
        "    #intrsctn = K.sum(intersection[1:])\n",
        "    #intrsctn = K.sum(intersection[1]+intersection[2]+intersection[4])\n",
        "    union = K.sum((y_true + y_pred), axis = (0,1,2)) - K.sum((y_true * y_pred), axis = (0,1,2))\n",
        "    #unin = K.sum(union[1:])\n",
        "    #unin = K.sum(union[1]+union[2]+union[4])\n",
        "    \n",
        "    iu = ((intersection + smooth)/ (union + smooth))\n",
        "    #iu = K.sum((intrsctn+smooth)/(unin+smooth))\n",
        "    return K.sum(iu[1:])#+iu[2]+iu[4]+iu[8]+iu[9])\n",
        "    \n",
        "\n",
        "def iou_loss(y_true, y_pred):\n",
        "    ncl = 9.0\n",
        "    return (ncl-iou(y_true, y_pred))\n",
        "\n",
        "\n",
        " \n",
        "def dice(y_true, y_pred):\n",
        "    smooth1 = 1e-12\n",
        "    '''\n",
        "    size = int(256)\n",
        "    y_true = K.cast(K.reshape(y_true, (-1,size,size)),np.uint8)\n",
        "    y_true = K.one_hot(y_true,3)\n",
        "    #class_weights = K.sum(y_true, axis = (0,1,2,3))/(K.sum(y_true, axis = (0,1,2,3)) + K.sum((1 -y_true), axis = (0,1,2,3)))\n",
        "    #y_pred = weighted_ypred(y_true,y_pred)\n",
        "    '''\n",
        "    #w1 = K.sum(y_true)\n",
        "    #w2 = K.sum(1-y_true) \n",
        "    num1 = K.sum((y_true *  y_pred), axis = (0,1,2))\n",
        "    dnm1 = K.sum((y_true +  y_pred), axis = (0,1,2))     \n",
        "    '''\n",
        "    num2 = K.sum(num1[1:])\n",
        "    dnm2 = K.sum(dnm1[1:])\n",
        "    '''\n",
        "    '''\n",
        "    num2 = K.sum(num1[1]+num1[2]+num1[4])\n",
        "    dnm2 = K.sum(dnm1[1]+dnm1[2]+dnm1[4])\n",
        "    '''\n",
        "    #num2 = K.sum(num1)\n",
        "    #dnm2 = K.sum(dnm1)\n",
        "\n",
        "\n",
        "    #num2 = K.sum(((1-y_true) * (1- y_pred)), axis = (0,1,2,3))\n",
        "    #dnm2 = K.sum((2- y_true -  y_pred), axis = (0,1,2,3))     \n",
        "    \n",
        "    f1 = ((2*num1 + smooth1)/ (dnm1 + smooth1)) #+ w1*((2*num2 + smooth1)/ (dnm2 + smooth1)))/(w1+w2)\n",
        "    #f1 = K.sum((2*num2 + smooth1)/ (dnm2 + smooth1))\n",
        "    return K.sum(f1[1:])#+f1[2]+f1[4]+f1[8]+f1[9])\n",
        " \n",
        "def dice_loss(y_true, y_pred):\n",
        "    ncl = 9.0\n",
        "    return (ncl-dice(y_true, y_pred))\n",
        "\n",
        "def comb_acc(y_true, y_pred):\n",
        "    return (dice(y_true, y_pred) + iou(y_true, y_pred))/2\n",
        "\n",
        "def comb_loss(y_true, y_pred):\n",
        "    return (dice_loss(y_true, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePUbRJ3KzuAa"
      },
      "source": [
        "size = 256\n",
        "def unet(size, lri, input_height = size, input_width = size, nClasses = 10):\n",
        "\n",
        "    K.set_image_data_format('channels_last')\n",
        "    \n",
        "    input_size = (input_width, input_height, 6)\n",
        "    input1 = Input(shape = input_size)\n",
        "    n = 64\n",
        "    drate1 = 0\n",
        "    drate2 = 0\n",
        "    drate3 = 0\n",
        "    drate4 = 0\n",
        "    drate5 = 0\n",
        "    lmbd = 0\n",
        "    #bn01 = ( BatchNormalization())(input1) \n",
        "    \n",
        "    #drop0 = Dropout(0)(input1)\n",
        "    conv1 = Conv2D(n, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer = 'he_normal')(input1)\n",
        "    drop1 = Dropout(drate1)(conv1)\n",
        "    bn11 = ( BatchNormalization())(drop1) \n",
        "\n",
        "    conv1 = Conv2D(n, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer = 'he_normal')(bn11)\n",
        "    drop12 = Dropout(drate2)(conv1)\n",
        "    bn12 = ( BatchNormalization())(drop12) \n",
        "\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(bn12)\n",
        "    \n",
        "    conv2 = Conv2D(n*2, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same',  data_format= \"channels_last\", kernel_initializer = 'he_normal')(pool1)\n",
        "    drop2 = Dropout(drate1)(conv2) \n",
        "    bn21 = ( BatchNormalization())(drop2)\n",
        "\n",
        "    conv2 = Conv2D(n*2, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same',  data_format= \"channels_last\", kernel_initializer = 'he_normal')(bn21)\n",
        "    drop22 = Dropout(drate2)(conv2)  \n",
        "    bn22 = ( BatchNormalization())(drop22)   \n",
        "    \n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(bn22)\n",
        "    \n",
        "    conv3 = Conv2D(n*4, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer = 'he_normal')(pool2)\n",
        "    drop3 = Dropout(drate1)(conv3)\n",
        "    bn31 = ( BatchNormalization())(drop3)\n",
        "\n",
        "    conv3 = Conv2D(n*4, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(bn31)\n",
        "    drop32 = Dropout(drate2)(conv3)\n",
        "    bn32 = ( BatchNormalization())(drop32) \n",
        "    \n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(bn32)\n",
        "    \n",
        "    conv4 = Conv2D(n*8, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(pool3)\n",
        "    drop4 = Dropout(drate1)(conv4)\n",
        "    bn41 = ( BatchNormalization())(drop4) \n",
        "\n",
        "    conv4 = Conv2D(n*8, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(bn41)\n",
        "    drop42 = Dropout(drate5)(conv4)\n",
        "    bn42 = ( BatchNormalization())(drop42) \n",
        "    \n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(bn42)\n",
        "   \n",
        "    conv5 = Conv2D(n*16, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(pool4)\n",
        "    drop5 = Dropout(drate5)(conv5)\n",
        "    bn51 = ( BatchNormalization())(drop5) \n",
        "    conv5 = Conv2D(n*16, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(bn51)\n",
        "    drop52 = Dropout(drate4)(conv5)\n",
        "    bn52 = ( BatchNormalization())(drop52) \n",
        "    \n",
        "    up6 = Conv2D(n*8, (2,2), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same',data_format= \"channels_last\",  kernel_initializer ='he_normal')(UpSampling2D(size = (2,2))(bn52))\n",
        "    merge6 = concatenate([bn42,up6], axis = 3)\n",
        "    #merge6 = concatenate([conv4,up6], axis = 3)\n",
        "    \n",
        "    conv6 = Conv2D(n*8, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(merge6)\n",
        "    drop6 = Dropout(drate3)(conv6)\n",
        "    bn61 = ( BatchNormalization())(drop6) \n",
        "\n",
        "    conv6 = Conv2D(n*8, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(bn61)\n",
        "    drop62 = Dropout(drate4)(conv6)\n",
        "    bn62 = ( BatchNormalization())(drop62) \n",
        "    \n",
        "    up7 = Conv2D(n*4, (2,2), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(UpSampling2D(size = (2,2))(bn62))\n",
        "    \n",
        "    merge7 = concatenate([bn32,up7], axis = 3)\n",
        "    \n",
        "    \n",
        "    conv7 = Conv2D(n*4, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(merge7)\n",
        "    drop7 = Dropout(drate3)(conv7)\n",
        "    bn71 = ( BatchNormalization())(drop7) \n",
        "\n",
        "    conv7 = Conv2D(n*4, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(bn71)\n",
        "    drop72 = Dropout(drate4)(conv7)\n",
        "    bn72 = ( BatchNormalization())(drop72) \n",
        "    \n",
        "\n",
        "    up8 = Conv2D(n*2, (2,2), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(UpSampling2D(size = (2,2))(bn72))\n",
        "    \n",
        "    \n",
        "    merge8 = concatenate([bn22,up8], axis = 3)\n",
        "    \n",
        "    conv8 = Conv2D(n*2, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(merge8)\n",
        "    drop8 = Dropout(drate3)(conv8)\n",
        "    bn81 = ( BatchNormalization())(drop8) \n",
        "\n",
        "    conv8 = Conv2D(n*2, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(bn81)\n",
        "    drop82 = Dropout(drate4)(conv8)\n",
        "    bn82 = ( BatchNormalization())(drop82) \n",
        "    \n",
        "    up9 = Conv2D(n, (2,2), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(UpSampling2D(size = (2,2))(bn82))\n",
        "    merge9 = concatenate([bn12,up9], axis = 3)\n",
        "    \n",
        "    conv9 = Conv2D(n, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(merge9)\n",
        "    drop9 = Dropout(drate3)(conv9)\n",
        "    bn91 = ( BatchNormalization())(drop9) \n",
        "\n",
        "    conv9 = Conv2D(n, (3,3), kernel_regularizer=l2(lmbd), activation = 'relu', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(bn91)\n",
        "    drop92 = Dropout(drate4)(conv9)\n",
        "    bn92 = ( BatchNormalization())(drop92) \n",
        "    \n",
        "    conv10 = Conv2D(nClasses, (1,1), kernel_regularizer=l2(lmbd), activation = 'softmax', padding = 'same', data_format= \"channels_last\", kernel_initializer ='he_normal')(bn92)\n",
        "    model = Model(inputs = input1, outputs = conv10)\n",
        "    #model.compile(optimizer = Adam(lr  = lri), loss= dice_loss, metrics = [iou, dice])\n",
        "    model.compile(optimizer = Adam(lr  = lri), loss= dice_loss, metrics = [iou, dice])\n",
        "   \n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htaFNBX4z3t-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9d0ea6ab-3b66-4e81-8591-de11686dc744"
      },
      "source": [
        "filepath1 = '/content/drive/My Drive/Excel/vietnam train images'\n",
        "filepath2 = '/content/drive/My Drive/Excel/vietnam train labels'\n",
        "\n",
        "#filepath1 = '/content/drive/My Drive/Excel/vietnam images'\n",
        "#filepath2 = '/content/drive/My Drive/Excel/vietnam pred final 4class4 processed'\n",
        "size = 256\n",
        "#List of all jps and tif files\n",
        "ipath = ((get_all_images(filepath1, 'img')))\n",
        "cpath = ((get_all_images(filepath2, 'img')))\n",
        "print(len(ipath))\n",
        "print(len(cpath))\n",
        "\n",
        "ipath_train = ipath\n",
        "cpath_train = cpath\n",
        "\n",
        "# ###ipath_train, ipath_test, cpath_train, cpath_test = train_test_split(ipath,cpath, train_size = 0.9, random_state=64, shuffle = True)\n",
        "# print(len(ipath_train), len(ipath_test))#, len(ipath_val), len(ipath_dev))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23\n",
            "23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2UvXO4L0C-Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ea186d3-d8d4-4efd-d7cf-dad630d21efa"
      },
      "source": [
        "idj = []\n",
        "ind = []\n",
        "ar1 = []\n",
        "ar2 = []\n",
        "\n",
        "\n",
        "for i in range(len(ipath_train)):\n",
        "    idj,ind,ar1,ar2 = apply_coord(ipath_train[i],idj,ind,ar1,ar2,1,i)\n",
        "    #numbr = (800*nmbr*num[i]).astype(np.uint32)\n",
        "    #idj,ind,ar1,ar2 = apply_rdata(ipath[i],idj,ind,ar1,ar2,numbr,i)\n",
        "    # idj,ind,ar1,ar2 = apply_rdata(ipath_train[i],idj,ind,ar1,ar2,10000,i)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7561 7694\n",
            "29 30 870\n",
            "(29,)\n",
            "(30,)\n",
            "idj:  870\n",
            "ar1:  870\n",
            "ar2:  870\n",
            "10980 7694\n",
            "42 30 1260\n",
            "(42,)\n",
            "(30,)\n",
            "idj:  1260\n",
            "ar1:  1260\n",
            "ar2:  1260\n",
            "10980 7694\n",
            "42 30 1260\n",
            "(42,)\n",
            "(30,)\n",
            "idj:  1260\n",
            "ar1:  1260\n",
            "ar2:  1260\n",
            "7561 10980\n",
            "29 42 1218\n",
            "(29,)\n",
            "(42,)\n",
            "idj:  1218\n",
            "ar1:  1218\n",
            "ar2:  1218\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 10980\n",
            "42 42 1764\n",
            "(42,)\n",
            "(42,)\n",
            "idj:  1764\n",
            "ar1:  1764\n",
            "ar2:  1764\n",
            "10980 4161\n",
            "42 16 672\n",
            "(42,)\n",
            "(16,)\n",
            "idj:  672\n",
            "ar1:  672\n",
            "ar2:  672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VYQAGqr0Ydg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "a2a2bc95-1a96-4040-c404-983bd8d3c3c2"
      },
      "source": [
        "def shuffle(idj,ind,ar1,ar2):\n",
        "    for ixd in range(5):\n",
        "        idj,ind,ar1,ar2 = shuffle_list(idj,ind,ar1,ar2, seeds = (204+ixd))\n",
        "\n",
        "    return (idj,ind,ar1,ar2)\n",
        "\n",
        "#idj0,ind0,ar10,ar20 = shuffel(idj0,ind0,ar10,ar20)\n",
        "\n",
        "ar1_train, ar1_dev, ar2_train, ar2_dev, ind_train, ind_dev, idj_train, idj_dev = train_test_split(ar1,ar2,ind,idj, train_size = 0.9, random_state=64, shuffle = True)#, stratify = idj)\n",
        "ar1_test, ar1_val, ar2_test, ar2_val, ind_test, ind_val, idj_test, idj_val = train_test_split(ar1_dev,ar2_dev,ind_dev,idj_dev, train_size = 0.5, random_state=64, shuffle = True)#, stratify = idj_dev)\n",
        "\n",
        "# idj_train,ind_train,ar1_train,ar2_train = shuffle(idj_train,ind_train,ar1_train,ar2_train)\n",
        "# idj_val,ind_val,ar1_val,ar2_val = shuffle(idj_val,ind_val,ar1_val,ar2_val)\n",
        "\n",
        "print('ar1_train: ', len(ar1_train))\n",
        "print('ar1_val: ', len(ar1_val))\n",
        "\n",
        "print(len(set(idj_train)))\n",
        "print(len(set(idj_val)))\n",
        "print(len(set(idj_test)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ar1_train:  33328\n",
            "ar1_val:  1852\n",
            "23\n",
            "23\n",
            "23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvylUZLQDVjL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "143dc792-9b0a-4c80-cfc5-0b5e9cc7d269"
      },
      "source": [
        "print(len(ar1_dev),len(ar2_dev),len(ind_dev),len(idj_dev))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3704 3704 3704 3704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f49huDX00eW4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "1fa95883-a1f0-4381-d240-3467b5f7cb7f"
      },
      "source": [
        "batch = 128\n",
        "val_batch = 32\n",
        "# train_generator = img_generator(ipath_train,cpath_train,idj_train, ind_train,ar1_train,ar2_train, batch)\n",
        "train_generator = msk_generator(ipath_train,cpath_train,idj_train, ind_train,ar1_train,ar2_train, batch)\n",
        "\n",
        "# %timeit time1 = train_generator\n",
        "\n",
        "\n",
        "val_generator = msk_generator(ipath_train,cpath_train,idj_val, ind_val,ar1_val,ar2_val, val_batch)\n",
        "\n",
        "#%timeit time1 = val_generator\n",
        "\n",
        "\n",
        "\n",
        "MIN_LR = 5e-5\n",
        "MAX_LR = 2e-4\n",
        "BATCH_SIZE = batch\n",
        "STEP_SIZE = 8\n",
        "CLR_METHOD = \"triangular\"\n",
        "NUM_EPOCHS = 96\n",
        "\n",
        "\n",
        "# clr = CyclicLR(\n",
        "# \tmode= CLR_METHOD,\n",
        "# \tbase_lr= MIN_LR,\n",
        "# \tmax_lr= MAX_LR,\n",
        "# \tstep_size=  STEP_SIZE * ((len(ar1_train)) // BATCH_SIZE))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:136: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (128, 256, 256, 6) (6 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:136: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (128, 256, 256, 10) (10 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:136: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (32, 256, 256, 6) (6 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:136: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (32, 256, 256, 10) (10 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PulM8cH60hx_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd394f30-9783-4ee6-d2b8-86837f9536d0"
      },
      "source": [
        "def training(size):\n",
        "    K.clear_session()\n",
        "    model = 0\n",
        "    del model         \n",
        "    K.set_image_data_format('channels_last')\n",
        "    seed = 100\n",
        "    np.random.seed(seed)\n",
        "    tf.compat.v1.set_random_seed(seed)\n",
        "    lri = 1e-4\n",
        "\n",
        "    model = unet(size, lri)\n",
        "    model.summary()\n",
        "    # model = load_model('/content/drive/My Drive/Excel/vietnam_unsup2-1.h5', custom_objects={'dice_loss': dice_loss,'iou': iou, 'dice': dice})\n",
        "    model.load_weights('/content/drive/My Drive/Excel/vietnam_unsup2-3.h5')\n",
        "    checkpoint = ModelCheckpoint(\"/content/drive/My Drive/Excel/vietnam_unsup2-4.h5\", monitor='val_dice', verbose=1, save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
        "    #logdir = os.path.join(logs_base_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    #tensorboard = TensorBoard(log_dir= logdir, histogram_freq=1)\n",
        "    early = EarlyStopping(monitor='val_iou', min_delta=0, patience=1000, verbose=1, mode='auto')\n",
        "    history = model.fit_generator(train_generator, steps_per_epoch = (len(ar1_train)//batch), epochs = 1000 ,verbose = 2, callbacks = [checkpoint, early], validation_data= val_generator, validation_steps = (len(ar1_val)//val_batch))    \n",
        "    #history = model.fit_generator(train_generator, steps_per_epoch = (len(train_X)/ batch), epochs =300 ,verbose = 2, callbacks = [tensorboard, checkpoint], validation_data= val_generator, validation_steps = (len(val_X)/4), shuffle =True)        \n",
        "    #for layer in model.layers[:6]:\n",
        "    #    layer.trainable = False\n",
        "    #model.compile(optimizer = Adam(lr  = lri), loss= dice_loss, metrics = [iou, dice])\n",
        "\n",
        "    # %timeit time1 = model.fit_generator(train_generator, steps_per_epoch = (len(ar1_train)//32), epochs = 1 ,verbose = 2, callbacks = [checkpoint, early], validation_data= val_generator, validation_steps = (len(ar1_val)//32))    \n",
        "    # history = model.fit(x = (arr[:tvsplit, :,:,:3]), y = (arr[:tvsplit, :,:,3:4]), batch_size = 20, epochs = 100 ,verbose = 2, callbacks = [tensorboard, checkpoint, early], validation_data= ((arr[tvsplit:vtsplit, :,:,:3]), (arr[tvsplit:vtsplit, :,:,3:4])), shuffle =True)        \n",
        "    # %timeit time2 = model.fit_generator(train_generator, steps_per_epoch = (len(ar1_train)//64), epochs = 1 ,verbose = 2, callbacks = [checkpoint, early], validation_data= val_generator, validation_steps = (len(ar1_val)//32))    \n",
        "    # %timeit time3 = model.fit_generator(train_generator, steps_per_epoch = (len(ar1_train)//128), epochs = 1 ,verbose = 2, callbacks = [checkpoint, early], validation_data= val_generator, validation_steps = (len(ar1_val)//32))    \n",
        "    # %timeit time4 = model.fit_generator(train_generator, steps_per_epoch = (len(ar1_train)//256), epochs = 1 ,verbose = 2, callbacks = [checkpoint, early], validation_data= val_generator, validation_steps = (len(ar1_val)//32))    \n",
        "    # %timeit time5 = model.fit_generator(train_generator, steps_per_epoch = (len(ar1_train)//512), epochs = 1 ,verbose = 2, callbacks = [checkpoint, early], validation_data= val_generator, validation_steps = (len(ar1_val)//32))    \n",
        "    # %timeit time6 = model.fit_generator(train_generator, steps_per_epoch = (len(ar1_train)//1024), epochs = 1 ,verbose = 2, callbacks = [checkpoint, early], validation_data= val_generator, validation_steps = (len(ar1_val)//32))    \n",
        "    # %timeit time7 = model.fit_generator(train_generator, steps_per_epoch = (len(ar1_train)//2048), epochs = 1 ,verbose = 2, callbacks = [checkpoint, early], validation_data= val_generator, validation_steps = (len(ar1_val)//32))    \n",
        "\n",
        "\n",
        "training(size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 256, 256, 6) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 256, 256, 64) 3520        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 256, 256, 64) 0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 256, 256, 64) 256         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 256, 256, 64) 36928       batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 256, 256, 64) 0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 256, 256, 64) 256         dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 128, 128, 128 0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128, 128, 128 512         dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 128, 128, 128 147584      batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 128, 128, 128 0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128, 128, 128 512         dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 64, 64, 256)  0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64, 64, 256)  1024        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 256)  590080      batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 64, 64, 256)  0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 64, 256)  1024        dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 512)  0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 512)  2048        dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 512)  2359808     batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 512)  0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 512)  2048        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 1024) 4719616     max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 16, 16, 1024) 0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 1024) 4096        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 1024) 9438208     batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 16, 16, 1024) 0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 1024) 4096        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 32, 32, 1024) 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 512)  2097664     up_sampling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 1024) 0           batch_normalization_7[0][0]      \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 512)  4719104     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 32, 32, 512)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 512)  2048        dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 512)  2359808     batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 512)  0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 512)  2048        dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 512)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 64, 64, 256)  524544      up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 64, 64, 512)  0           batch_normalization_5[0][0]      \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 64, 64, 256)  1179904     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 64, 64, 256)  0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 64, 64, 256)  1024        dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 64, 64, 256)  590080      batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 64, 64, 256)  0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 64, 64, 256)  1024        dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 256 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 128, 128, 128 131200      up_sampling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 128, 128, 256 0           batch_normalization_3[0][0]      \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 128, 128, 128 295040      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 128, 128, 128 0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 128, 128, 128 512         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 128, 128, 128 147584      batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 128, 128, 128 0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 128, 128, 128 512         dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 128 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 256, 256, 64) 32832       up_sampling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 256, 256, 128 0           batch_normalization_1[0][0]      \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 256, 256, 64) 0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 256, 256, 64) 256         dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 256, 256, 64) 36928       batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 256, 256, 64) 0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 256, 256, 64) 256         dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 256, 256, 10) 650         batch_normalization_17[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 31,057,610\n",
            "Trainable params: 31,045,834\n",
            "Non-trainable params: 11,776\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "WARNING:tensorflow:From <ipython-input-16-f0fbf32e7b46>:19: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0403s vs `on_train_batch_end` time: 0.0645s). Check your callbacks.\n",
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_test_batch_end` time: 0.0183s). Check your callbacks.\n",
            "\n",
            "Epoch 00001: val_dice improved from -inf to 5.59934, saving model to /content/drive/My Drive/Excel/vietnam_unsup2-4.h5\n",
            "260/260 - 33s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.4007 - val_iou: 5.5652 - val_dice: 5.5993\n",
            "Epoch 2/1000\n",
            "\n",
            "Epoch 00002: val_dice improved from 5.59934 to 5.71402, saving model to /content/drive/My Drive/Excel/vietnam_unsup2-4.h5\n",
            "260/260 - 34s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.2860 - val_iou: 5.6833 - val_dice: 5.7140\n",
            "Epoch 3/1000\n",
            "\n",
            "Epoch 00003: val_dice improved from 5.71402 to 5.75925, saving model to /content/drive/My Drive/Excel/vietnam_unsup2-4.h5\n",
            "260/260 - 34s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.2407 - val_iou: 5.7319 - val_dice: 5.7593\n",
            "Epoch 4/1000\n",
            "\n",
            "Epoch 00004: val_dice did not improve from 5.75925\n",
            "260/260 - 31s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.3211 - val_iou: 5.6482 - val_dice: 5.6789\n",
            "Epoch 5/1000\n",
            "\n",
            "Epoch 00005: val_dice did not improve from 5.75925\n",
            "260/260 - 31s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.4189 - val_iou: 5.5469 - val_dice: 5.5811\n",
            "Epoch 6/1000\n",
            "\n",
            "Epoch 00006: val_dice improved from 5.75925 to 5.97325, saving model to /content/drive/My Drive/Excel/vietnam_unsup2-4.h5\n",
            "260/260 - 33s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.0268 - val_iou: 5.9528 - val_dice: 5.9732\n",
            "Epoch 7/1000\n",
            "\n",
            "Epoch 00007: val_dice did not improve from 5.97325\n",
            "260/260 - 30s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.3386 - val_iou: 5.6307 - val_dice: 5.6614\n",
            "Epoch 8/1000\n",
            "\n",
            "Epoch 00008: val_dice did not improve from 5.97325\n",
            "260/260 - 30s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.4992 - val_iou: 5.4632 - val_dice: 5.5008\n",
            "Epoch 9/1000\n",
            "\n",
            "Epoch 00009: val_dice did not improve from 5.97325\n",
            "260/260 - 30s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.2232 - val_iou: 5.7495 - val_dice: 5.7768\n",
            "Epoch 10/1000\n",
            "\n",
            "Epoch 00010: val_dice did not improve from 5.97325\n",
            "260/260 - 30s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.3035 - val_iou: 5.6657 - val_dice: 5.6965\n",
            "Epoch 11/1000\n",
            "\n",
            "Epoch 00011: val_dice did not improve from 5.97325\n",
            "260/260 - 30s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.1429 - val_iou: 5.8332 - val_dice: 5.8571\n",
            "Epoch 12/1000\n",
            "\n",
            "Epoch 00012: val_dice did not improve from 5.97325\n",
            "260/260 - 30s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.4634 - val_iou: 5.4990 - val_dice: 5.5366\n",
            "Epoch 13/1000\n",
            "\n",
            "Epoch 00013: val_dice did not improve from 5.97325\n",
            "260/260 - 30s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.3386 - val_iou: 5.6307 - val_dice: 5.6614\n",
            "Epoch 14/1000\n",
            "\n",
            "Epoch 00014: val_dice did not improve from 5.97325\n",
            "260/260 - 30s - loss: 1.0147 - iou: 7.9708 - dice: 7.9852 - val_loss: 3.3386 - val_iou: 5.6307 - val_dice: 5.6614\n",
            "Epoch 15/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f0fbf32e7b46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-f0fbf32e7b46>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#tensorboard = TensorBoard(log_dir= logdir, histogram_freq=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mearly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_iou'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mval_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m#history = model.fit_generator(train_generator, steps_per_epoch = (len(train_X)/ batch), epochs =300 ,verbose = 2, callbacks = [tensorboard, checkpoint], validation_data= val_generator, validation_steps = (len(val_X)/4), shuffle =True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#for layer in model.layers[:6]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdEvTfbUGA6k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "ef7429ff-7ef3-424f-a4aa-9ce1d4609f19"
      },
      "source": [
        "tr_generator = msk_generator(ipath_train,cpath_train,idj_train, ind_train,ar1_train,ar2_train, val_batch)\n",
        "test_generator = msk_generator(ipath_train,cpath_train,idj_test, ind_test,ar1_test,ar2_test, val_batch)\n",
        "lri = 1e-4\n",
        "model = unet(size, lri)\n",
        "model.load_weights(\"/content/drive/My Drive/Excel/vietnam_kmeans_gpu_label1.h5\")\n",
        "# model = load_model('/content/drive/My Drive/Excel/vietnam_unsup2-3.h5', custom_objects={'dice_loss': dice_loss,'iou': iou, 'dice': dice})\n",
        "#print(l0,i0,d0)\n",
        "print('set', model.metrics_names)  \n",
        "l,i,d = model.evaluate_generator(tr_generator, verbose=1, steps=(len(ar1_train)//val_batch))\n",
        "print('train: ',l,i,d)\n",
        "\n",
        "l,i,d = model.evaluate_generator(val_generator, verbose=1, steps=(len(ar1_val)//val_batch))\n",
        "print('val: ',l,i,d)\n",
        "\n",
        "l,i,d = model.evaluate_generator(test_generator, verbose=1, steps=(len(ar1_test)//val_batch))\n",
        "print('test: ',l,i,d)\n",
        "\n",
        "   \n",
        "K.clear_session()\n",
        "del model \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:136: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (32, 256, 256, 6) (6 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:136: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (32, 256, 256, 10) (10 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:136: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (32, 256, 256, 6) (6 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:136: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (32, 256, 256, 10) (10 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "set ['loss', 'iou', 'dice']\n",
            "   1/1041 [..............................] - ETA: 40s - loss: 1.0147 - iou: 7.9708 - dice: 7.9853WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_test_batch_end` time: 0.0344s). Check your callbacks.\n",
            "1041/1041 [==============================] - 31s 30ms/step - loss: 1.0148 - iou: 7.9707 - dice: 7.9853\n",
            "train:  1.014756679534912 7.97073221206665 7.98532247543335\n",
            " 1/57 [..............................] - ETA: 0s - loss: 3.0003 - iou: 5.9997 - dice: 5.9997WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_test_batch_end` time: 0.0200s). Check your callbacks.\n",
            "57/57 [==============================] - 2s 29ms/step - loss: 3.3386 - iou: 5.6307 - dice: 5.6614\n",
            "val:  3.3386118412017822 5.6306562423706055 5.6613874435424805\n",
            " 1/57 [..............................] - ETA: 0s - loss: 3.0099 - iou: 5.9901 - dice: 5.9901WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_test_batch_end` time: 0.0180s). Check your callbacks.\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 3.0099 - iou: 5.9901 - dice: 5.9901\n",
            "test:  3.009887933731079 5.990111827850342 5.990111827850342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTOTygDIUMbk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}